{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04d0dc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "class Config:\n",
    "    DATA_PATH = \"data/dataset_yolo_cropped\" \n",
    "    CLASSES = [\"Большой Желтохохлый Какаду\", \"Буроухий Краснохвостый Попугай\", \"Волнистый Попугайчик\",\n",
    "               #\"Гологлазый Какаду\", \"Зеленокрылый Ара\", \"Индийский кольчатый попугай\",\n",
    "               \"Корелла\", \"Королевский Попугай\", \"Красная Розелла\",\n",
    "               #\"Краснохвостый Траурный Какаду\", \"Красный Ара\", \"Розовощёкий Неразлучник\", \"Розовый Какаду\",\n",
    "               \"Сине-жёлтый Ара\", \"Украшенный Лорикет\", \"Черноголовый Попугай\"]\n",
    "    IMG_SIZE = 224\n",
    "    \n",
    "    ARCHITECTURE = \"ImprovedAlexNet\"     \n",
    "    PRETRAINED = False   \n",
    "    MODEL_NAME = \"ImprovedAlexNet_YOLO_6\"\n",
    "    SAVE_PATH = f\"results/models/{MODEL_NAME}.pth\"           \n",
    "    LEARNING_CURVES_PATH = f\"results/learning_curves/{MODEL_NAME}.png\"\n",
    "\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 75\n",
    "    LR = 0.001\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c65cf569",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomResizeTransform:\n",
    "    def __init__(self, target_size):\n",
    "        self.target_size = target_size\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        # Получаем размеры исходного изображения\n",
    "        width, height = img.size\n",
    "        \n",
    "        # Если обе стороны меньше целевого размера - просто растягиваем\n",
    "        if width < self.target_size and height < self.target_size:\n",
    "            return img.resize((self.target_size, self.target_size), Image.BILINEAR)\n",
    "        \n",
    "        # Если одна из сторон меньше - растягиваем меньшую сторону до target_size\n",
    "        # и сохраняем пропорции\n",
    "        elif width < self.target_size or height < self.target_size:\n",
    "            if width < height:\n",
    "                new_width = self.target_size\n",
    "                new_height = int(height * (self.target_size / width))\n",
    "            else:\n",
    "                new_height = self.target_size\n",
    "                new_width = int(width * (self.target_size / height))\n",
    "            img = img.resize((new_width, new_height), Image.BILINEAR)\n",
    "        \n",
    "        # Теперь делаем центральную обрезку до target_size x target_size\n",
    "        width, height = img.size\n",
    "        left = (width - self.target_size)/2\n",
    "        top = (height - self.target_size)/2\n",
    "        right = (width + self.target_size)/2\n",
    "        bottom = (height + self.target_size)/2\n",
    "        \n",
    "        return img.crop((left, top, right, bottom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adfeefdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    CustomResizeTransform(Config.IMG_SIZE),  # Ресайз\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Только горизонтальный flip\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Без сильных искажений\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),  # ImageNet-нормализация\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4020e28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, 11, stride=4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(3, 2),\n",
    "            nn.Conv2d(96, 256, 5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(3, 2),\n",
    "            nn.Conv2d(256, 384, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 384, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((6, 6))\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256*6*6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, len(Config.CLASSES)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), 256*6*6)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94ae4e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedAlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, 11, stride=4),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(3, 2),\n",
    "            \n",
    "            nn.Conv2d(96, 256, 5, padding=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(3, 2),\n",
    "            \n",
    "            nn.Conv2d(256, 384, 3, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(384, 384, 3, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(384, 512, 3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.AdaptiveAvgPool2d((6, 6))\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512*6*6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(4096),\n",
    "            \n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(4096),\n",
    "            \n",
    "            nn.Linear(4096, len(Config.CLASSES)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), 512*6*6)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41cce2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # Загружаем все данные (включая лишние классы)\n",
    "    full_dataset = datasets.ImageFolder(Config.DATA_PATH, transform=train_transform)\n",
    "    \n",
    "    # Оставляем только samples с нужными классами\n",
    "    valid_indices = [\n",
    "        i for i, (path, label) in enumerate(full_dataset.samples)\n",
    "        if full_dataset.classes[label] in Config.CLASSES\n",
    "    ]\n",
    "    dataset = torch.utils.data.Subset(full_dataset, valid_indices)\n",
    "    \n",
    "    # Обновляем классы (чтобы метки были 0, 1, 2... без пропусков)\n",
    "    old_class_to_idx = {cls: idx for idx, cls in enumerate(full_dataset.classes)}\n",
    "    new_class_to_idx = {cls: idx for idx, cls in enumerate(Config.CLASSES)}\n",
    "    \n",
    "    for i in dataset.indices:\n",
    "        path, old_label = full_dataset.samples[i]\n",
    "        cls = full_dataset.classes[old_label]\n",
    "        full_dataset.samples[i] = (path, new_class_to_idx[cls])\n",
    "    \n",
    "    # Разделяем на train/val\n",
    "    train_size = int(0.9 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    return random_split(dataset, [train_size, val_size])\n",
    "\n",
    "def init_model():\n",
    "    if Config.ARCHITECTURE == \"AlexNet\":\n",
    "        model = AlexNet()\n",
    "    elif Config.ARCHITECTURE == \"ImprovedAlexNet\":\n",
    "        model = ImprovedAlexNet()\n",
    "    return model.to(Config.DEVICE)\n",
    "\n",
    "def train():\n",
    "    train_set, val_set = load_data()\n",
    "    val_loader = DataLoader(val_set, batch_size=Config.BATCH_SIZE)\n",
    "    \n",
    "    model = init_model()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=Config.LR)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Инициализируем шедулер\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='max',     # Отслеживаем рост val_acc\n",
    "        factor=0.5,     # Уменьшаем LR в 2 раза при отсутствии улучшений\n",
    "        patience=5,     # Ждём 5 эпох без улучшений\n",
    "        verbose=True    # Выводим сообщения об изменении LR\n",
    "    )\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    history = {'train_acc': [], 'val_acc': []}\n",
    "    \n",
    "    for epoch in range(Config.EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for inputs, labels in DataLoader(train_set, batch_size=Config.BATCH_SIZE, shuffle=True):\n",
    "            inputs = inputs.to(Config.DEVICE)\n",
    "            labels = labels.to(Config.DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            train_correct += (preds == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "        \n",
    "        train_acc = train_correct / train_total\n",
    "        history['train_acc'].append(train_acc)\n",
    "        \n",
    "        # Валидация\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(Config.DEVICE)\n",
    "                labels = labels.to(Config.DEVICE)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "        \n",
    "        val_acc = val_correct / val_total\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Обновляем шедулер на основе val_acc\n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), Config.SAVE_PATH)\n",
    "        \n",
    "        # Выводим текущий LR\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{Config.EPOCHS} | \"\n",
    "            f\"Train Loss: {running_loss/len(train_set):.4f} | \"\n",
    "            f\"Train Acc: {train_acc:.4f} | \"\n",
    "            f\"Val Loss: {val_loss/len(val_set):.4f} | \"\n",
    "            f\"Val Acc: {val_acc:.4f} | \"\n",
    "            f\"LR: {current_lr:.6f}\"  # Добавили вывод LR\n",
    "        )\n",
    "    \n",
    "    # Сохранение кривых обучения\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(history['train_acc'], label='Train Acc')\n",
    "    plt.plot(history['val_acc'], label='Val Acc')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    os.makedirs(os.path.dirname(Config.LEARNING_CURVES_PATH), exist_ok=True)\n",
    "    plt.savefig(Config.LEARNING_CURVES_PATH)\n",
    "    plt.close()\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a28443ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75 | Train Loss: 0.0758 | Train Acc: 0.2852 | Val Loss: 0.0612 | Val Acc: 0.3524 | LR: 0.001000\n",
      "Epoch 2/75 | Train Loss: 0.0592 | Train Acc: 0.3830 | Val Loss: 0.0582 | Val Acc: 0.3819 | LR: 0.001000\n",
      "Epoch 3/75 | Train Loss: 0.0558 | Train Acc: 0.4406 | Val Loss: 0.0578 | Val Acc: 0.4469 | LR: 0.001000\n",
      "Epoch 4/75 | Train Loss: 0.0534 | Train Acc: 0.4647 | Val Loss: 0.0517 | Val Acc: 0.4685 | LR: 0.001000\n",
      "Epoch 5/75 | Train Loss: 0.0481 | Train Acc: 0.5137 | Val Loss: 0.0427 | Val Acc: 0.5886 | LR: 0.001000\n",
      "Epoch 6/75 | Train Loss: 0.0425 | Train Acc: 0.5791 | Val Loss: 0.0396 | Val Acc: 0.6063 | LR: 0.001000\n",
      "Epoch 7/75 | Train Loss: 0.0399 | Train Acc: 0.5848 | Val Loss: 0.0443 | Val Acc: 0.5709 | LR: 0.001000\n",
      "Epoch 8/75 | Train Loss: 0.0388 | Train Acc: 0.5999 | Val Loss: 0.0379 | Val Acc: 0.5945 | LR: 0.001000\n",
      "Epoch 9/75 | Train Loss: 0.0395 | Train Acc: 0.5872 | Val Loss: 0.0410 | Val Acc: 0.5374 | LR: 0.001000\n",
      "Epoch 10/75 | Train Loss: 0.0397 | Train Acc: 0.5953 | Val Loss: 0.0495 | Val Acc: 0.5157 | LR: 0.001000\n",
      "Epoch 11/75 | Train Loss: 0.0338 | Train Acc: 0.6419 | Val Loss: 0.0320 | Val Acc: 0.6398 | LR: 0.001000\n",
      "Epoch 12/75 | Train Loss: 0.0333 | Train Acc: 0.6603 | Val Loss: 0.0332 | Val Acc: 0.6594 | LR: 0.001000\n",
      "Epoch 13/75 | Train Loss: 0.0368 | Train Acc: 0.6238 | Val Loss: 0.0601 | Val Acc: 0.5768 | LR: 0.001000\n",
      "Epoch 14/75 | Train Loss: 0.0341 | Train Acc: 0.6516 | Val Loss: 0.0305 | Val Acc: 0.6752 | LR: 0.001000\n",
      "Epoch 15/75 | Train Loss: 0.0351 | Train Acc: 0.6485 | Val Loss: 0.2297 | Val Acc: 0.1732 | LR: 0.001000\n",
      "Epoch 16/75 | Train Loss: 0.0448 | Train Acc: 0.5246 | Val Loss: 0.2743 | Val Acc: 0.5748 | LR: 0.001000\n",
      "Epoch 17/75 | Train Loss: 0.0348 | Train Acc: 0.6378 | Val Loss: 0.2908 | Val Acc: 0.5866 | LR: 0.001000\n",
      "Epoch 18/75 | Train Loss: 0.0331 | Train Acc: 0.6513 | Val Loss: 0.0324 | Val Acc: 0.6831 | LR: 0.001000\n",
      "Epoch 19/75 | Train Loss: 0.0298 | Train Acc: 0.6826 | Val Loss: 2.3183 | Val Acc: 0.6909 | LR: 0.001000\n",
      "Epoch 20/75 | Train Loss: 0.0303 | Train Acc: 0.6763 | Val Loss: 1.1094 | Val Acc: 0.6969 | LR: 0.001000\n",
      "Epoch 21/75 | Train Loss: 0.0307 | Train Acc: 0.6708 | Val Loss: 0.6075 | Val Acc: 0.6220 | LR: 0.001000\n",
      "Epoch 22/75 | Train Loss: 0.0323 | Train Acc: 0.6570 | Val Loss: 1.7727 | Val Acc: 0.6496 | LR: 0.001000\n",
      "Epoch 23/75 | Train Loss: 0.0282 | Train Acc: 0.6973 | Val Loss: 0.1771 | Val Acc: 0.7047 | LR: 0.001000\n",
      "Epoch 24/75 | Train Loss: 0.0261 | Train Acc: 0.7205 | Val Loss: 2.2365 | Val Acc: 0.6969 | LR: 0.001000\n",
      "Epoch 25/75 | Train Loss: 0.0253 | Train Acc: 0.7293 | Val Loss: 0.2360 | Val Acc: 0.6831 | LR: 0.001000\n",
      "Epoch 26/75 | Train Loss: 0.0249 | Train Acc: 0.7389 | Val Loss: 0.4732 | Val Acc: 0.6417 | LR: 0.001000\n",
      "Epoch 27/75 | Train Loss: 0.0235 | Train Acc: 0.7422 | Val Loss: 11.2811 | Val Acc: 0.7146 | LR: 0.001000\n",
      "Epoch 28/75 | Train Loss: 0.0258 | Train Acc: 0.7242 | Val Loss: 0.8254 | Val Acc: 0.5965 | LR: 0.001000\n",
      "Epoch 29/75 | Train Loss: 0.0304 | Train Acc: 0.6791 | Val Loss: 0.3700 | Val Acc: 0.6752 | LR: 0.001000\n",
      "Epoch 30/75 | Train Loss: 0.0244 | Train Acc: 0.7360 | Val Loss: 6.6576 | Val Acc: 0.7264 | LR: 0.001000\n",
      "Epoch 31/75 | Train Loss: 0.0348 | Train Acc: 0.6384 | Val Loss: 1.4586 | Val Acc: 0.5531 | LR: 0.001000\n",
      "Epoch 32/75 | Train Loss: 0.0339 | Train Acc: 0.6378 | Val Loss: 5.5432 | Val Acc: 0.6654 | LR: 0.001000\n",
      "Epoch 33/75 | Train Loss: 0.0268 | Train Acc: 0.7089 | Val Loss: 0.1862 | Val Acc: 0.7067 | LR: 0.001000\n",
      "Epoch 34/75 | Train Loss: 0.0240 | Train Acc: 0.7378 | Val Loss: 6.0281 | Val Acc: 0.6240 | LR: 0.001000\n",
      "Epoch 35/75 | Train Loss: 0.0238 | Train Acc: 0.7422 | Val Loss: 6.0292 | Val Acc: 0.7343 | LR: 0.001000\n",
      "Epoch 36/75 | Train Loss: 0.0231 | Train Acc: 0.7492 | Val Loss: 0.3109 | Val Acc: 0.7402 | LR: 0.001000\n",
      "Epoch 37/75 | Train Loss: 0.0254 | Train Acc: 0.7258 | Val Loss: 5.6624 | Val Acc: 0.7382 | LR: 0.001000\n",
      "Epoch 38/75 | Train Loss: 0.0254 | Train Acc: 0.7269 | Val Loss: 16.8484 | Val Acc: 0.6831 | LR: 0.001000\n",
      "Epoch 39/75 | Train Loss: 0.0235 | Train Acc: 0.7468 | Val Loss: 0.3082 | Val Acc: 0.7205 | LR: 0.001000\n",
      "Epoch 40/75 | Train Loss: 0.0200 | Train Acc: 0.7851 | Val Loss: 3.3633 | Val Acc: 0.7618 | LR: 0.001000\n",
      "Epoch 41/75 | Train Loss: 0.0204 | Train Acc: 0.7772 | Val Loss: 9.4858 | Val Acc: 0.7343 | LR: 0.001000\n",
      "Epoch 42/75 | Train Loss: 0.0206 | Train Acc: 0.7757 | Val Loss: 0.5387 | Val Acc: 0.7067 | LR: 0.001000\n",
      "Epoch 43/75 | Train Loss: 0.0205 | Train Acc: 0.7833 | Val Loss: 7.3588 | Val Acc: 0.7421 | LR: 0.001000\n",
      "Epoch 44/75 | Train Loss: 0.0191 | Train Acc: 0.7892 | Val Loss: 25.5839 | Val Acc: 0.5728 | LR: 0.001000\n",
      "Epoch 45/75 | Train Loss: 0.0217 | Train Acc: 0.7757 | Val Loss: 49.5885 | Val Acc: 0.5591 | LR: 0.001000\n",
      "Epoch 46/75 | Train Loss: 0.0243 | Train Acc: 0.7466 | Val Loss: 2.3248 | Val Acc: 0.6949 | LR: 0.000500\n",
      "Epoch 47/75 | Train Loss: 0.0205 | Train Acc: 0.7824 | Val Loss: 14.1833 | Val Acc: 0.7756 | LR: 0.000500\n",
      "Epoch 48/75 | Train Loss: 0.0178 | Train Acc: 0.8083 | Val Loss: 2.0733 | Val Acc: 0.7638 | LR: 0.000500\n",
      "Epoch 49/75 | Train Loss: 0.0166 | Train Acc: 0.8203 | Val Loss: 3.4422 | Val Acc: 0.7618 | LR: 0.000500\n",
      "Epoch 50/75 | Train Loss: 0.0165 | Train Acc: 0.8188 | Val Loss: 1.2028 | Val Acc: 0.7697 | LR: 0.000500\n",
      "Epoch 51/75 | Train Loss: 0.0155 | Train Acc: 0.8315 | Val Loss: 0.0547 | Val Acc: 0.7933 | LR: 0.000500\n",
      "Epoch 52/75 | Train Loss: 0.0147 | Train Acc: 0.8450 | Val Loss: 0.7396 | Val Acc: 0.7815 | LR: 0.000500\n",
      "Epoch 53/75 | Train Loss: 0.0142 | Train Acc: 0.8479 | Val Loss: 0.4215 | Val Acc: 0.7815 | LR: 0.000500\n",
      "Epoch 54/75 | Train Loss: 0.0131 | Train Acc: 0.8601 | Val Loss: 0.1009 | Val Acc: 0.8012 | LR: 0.000500\n",
      "Epoch 55/75 | Train Loss: 0.0132 | Train Acc: 0.8531 | Val Loss: 1.1674 | Val Acc: 0.7933 | LR: 0.000500\n",
      "Epoch 56/75 | Train Loss: 0.0128 | Train Acc: 0.8573 | Val Loss: 11.6666 | Val Acc: 0.7953 | LR: 0.000500\n",
      "Epoch 57/75 | Train Loss: 0.0113 | Train Acc: 0.8726 | Val Loss: 1.9301 | Val Acc: 0.8071 | LR: 0.000500\n",
      "Epoch 58/75 | Train Loss: 0.0114 | Train Acc: 0.8717 | Val Loss: 2.0003 | Val Acc: 0.8169 | LR: 0.000500\n",
      "Epoch 59/75 | Train Loss: 0.0122 | Train Acc: 0.8663 | Val Loss: 5.1455 | Val Acc: 0.7520 | LR: 0.000500\n",
      "Epoch 60/75 | Train Loss: 0.0125 | Train Acc: 0.8674 | Val Loss: 1.5240 | Val Acc: 0.7953 | LR: 0.000500\n",
      "Epoch 61/75 | Train Loss: 0.0105 | Train Acc: 0.8851 | Val Loss: 0.7947 | Val Acc: 0.7933 | LR: 0.000500\n",
      "Epoch 62/75 | Train Loss: 0.0107 | Train Acc: 0.8794 | Val Loss: 0.1321 | Val Acc: 0.8051 | LR: 0.000500\n",
      "Epoch 63/75 | Train Loss: 0.0119 | Train Acc: 0.8704 | Val Loss: 5.3303 | Val Acc: 0.6929 | LR: 0.000500\n",
      "Epoch 64/75 | Train Loss: 0.0130 | Train Acc: 0.8566 | Val Loss: 0.2989 | Val Acc: 0.7736 | LR: 0.000250\n",
      "Epoch 65/75 | Train Loss: 0.0125 | Train Acc: 0.8606 | Val Loss: 1.2052 | Val Acc: 0.7795 | LR: 0.000250\n",
      "Epoch 66/75 | Train Loss: 0.0118 | Train Acc: 0.8715 | Val Loss: 6.5166 | Val Acc: 0.7559 | LR: 0.000250\n",
      "Epoch 67/75 | Train Loss: 0.0108 | Train Acc: 0.8805 | Val Loss: 2.9046 | Val Acc: 0.7894 | LR: 0.000250\n",
      "Epoch 68/75 | Train Loss: 0.0092 | Train Acc: 0.9004 | Val Loss: 0.2854 | Val Acc: 0.8110 | LR: 0.000250\n",
      "Epoch 69/75 | Train Loss: 0.0097 | Train Acc: 0.8893 | Val Loss: 3.6710 | Val Acc: 0.7874 | LR: 0.000250\n",
      "Epoch 70/75 | Train Loss: 0.0089 | Train Acc: 0.9022 | Val Loss: 0.5153 | Val Acc: 0.7992 | LR: 0.000125\n",
      "Epoch 71/75 | Train Loss: 0.0085 | Train Acc: 0.9050 | Val Loss: 0.1241 | Val Acc: 0.8130 | LR: 0.000125\n",
      "Epoch 72/75 | Train Loss: 0.0082 | Train Acc: 0.9068 | Val Loss: 5.3408 | Val Acc: 0.8012 | LR: 0.000125\n",
      "Epoch 73/75 | Train Loss: 0.0074 | Train Acc: 0.9164 | Val Loss: 2.0615 | Val Acc: 0.7874 | LR: 0.000125\n",
      "Epoch 74/75 | Train Loss: 0.0074 | Train Acc: 0.9155 | Val Loss: 1.9715 | Val Acc: 0.7815 | LR: 0.000125\n",
      "Epoch 75/75 | Train Loss: 0.0075 | Train Acc: 0.9195 | Val Loss: 2.3106 | Val Acc: 0.8012 | LR: 0.000125\n"
     ]
    }
   ],
   "source": [
    "history = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aeca38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
